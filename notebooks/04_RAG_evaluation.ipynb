{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e14eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm \n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# docker run -p 6333:6333 -p 6334:6334 -v \"${PWD}\\qdrant_storage:/qdrant/storage\" qdrant/qdrant\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "\n",
    "OUTPUT_DIR = \"../data/experiments_output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dae4edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "poi_data = pd.read_csv('../data/krakow_pois_selected.csv')\n",
    "documents = poi_data.to_dict(orient='records')\n",
    "\n",
    "df_question = pd.read_csv(\"../data/ground-truth-retrieval.csv\")\n",
    "ground_truth = df_question.to_dict(orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f3c81ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = ['name','amenity','leisure','natural','tourism','historic','wiki_summary_en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f8fdb8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qdrant_client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "411bf6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSIONALITY = 512\n",
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c05b757a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_client.delete_collection('hybrid_search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1412cba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=\"hybrid_search\",\n",
    "    vectors_config={\n",
    "        # Named dense vector for jinaai/jina-embeddings-v2-small-en\n",
    "        \"jina-small\": models.VectorParams(\n",
    "            size=512,\n",
    "            distance=models.Distance.COSINE,\n",
    "        ),\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF,\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7998b01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_client.upsert(\n",
    "    collection_name=\"hybrid_search\",\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=doc['id'],\n",
    "            vector={\n",
    "                \"jina-small\": models.Document(\n",
    "                    text=doc['name'] + ' ' + doc['amenity'] + ' ' + doc['leisure'] + ' ' + doc['natural'] + ' ' + doc['tourism'] + ' ' + doc['historic'] + ' ' + doc['wiki_summary_en'],\n",
    "                    model=\"jinaai/jina-embeddings-v2-small-en\",\n",
    "                ),\n",
    "                \"bm25\": models.Document(\n",
    "                    text=doc['name'] + ' ' + doc['amenity'] + ' ' + doc['leisure'] + ' ' + doc['natural'] + ' ' + doc['tourism'] + ' ' + doc['historic'] + ' ' + doc['wiki_summary_en'],\n",
    "                    model=\"Qdrant/bm25\",\n",
    "                ),\n",
    "            },\n",
    "            payload={\n",
    "                \"name\": doc['name'],\n",
    "            \"wiki_summary_en\": doc['wiki_summary_en'],\n",
    "            'id'    : doc['id'],\n",
    "            }\n",
    "        )\n",
    "        for doc in documents\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0aaac5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rrf_search(query: str, limit: int = 1) -> list[models.ScoredPoint]:\n",
    "    results = qdrant_client.query_points(\n",
    "        collection_name=\"hybrid_search\",\n",
    "        prefetch=[\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=\"jinaai/jina-embeddings-v2-small-en\",\n",
    "                ),\n",
    "                using=\"jina-small\",\n",
    "                limit=(5 * limit),\n",
    "            ),\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=\"Qdrant/bm25\",\n",
    "                ),\n",
    "                using=\"bm25\",\n",
    "                limit=(5 * limit),\n",
    "            ),\n",
    "        ],\n",
    "        # Fusion query enables fusion on the prefetched results\n",
    "        query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "        with_payload=True,\n",
    "    )\n",
    "\n",
    "    return results.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dca58c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "76e28244",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_template = \"\"\"\n",
    "\n",
    "phone : {phone}\n",
    "cemetery : {cemetery}\n",
    "emergency : {emergency}\n",
    "opening_hours : {opening_hours}\n",
    "website : {website}\n",
    "pets_allowed : {pets_allowed}\n",
    "geometry : {geometry}\n",
    "historic : {historic}\n",
    "wiki_summary_en : {wiki_summary_en}\n",
    "postal_code : {postal_code}\n",
    "toilets : {toilets}\n",
    "natural : {natural}\n",
    "description : {description}\n",
    "visiting_time : {visiting_time}\n",
    "leisure : {leisure}\n",
    "tourism : {tourism}\n",
    "public_transport : {public_transport}\n",
    "brand : {brand}\n",
    "alt_name : {alt_name}\n",
    "amenity : {amenity}\n",
    "reservation : {reservation}\n",
    "attraction : {attraction}\n",
    "highchair : {highchair}\n",
    "parking : {parking}\n",
    "swimming_pool : {swimming_pool}\n",
    "contact_phone : {contact_phone}\n",
    "community_centre : {community_centre}\n",
    "addr_street : {addr_street}\n",
    "contact_twitter : {contact_twitter}\n",
    "social_facility : {social_facility}\n",
    "contact_facebook : {contact_facebook}\n",
    "zoo : {zoo}\n",
    "email : {email}\n",
    "wheelchair : {wheelchair}\n",
    "cuisine : {cuisine}\n",
    "contact_website : {contact_website}\n",
    "internet_access : {internet_access}\n",
    "opening_hours_reception : {opening_hours_reception}\n",
    "guest_house : {guest_house}\n",
    "addr_city : {addr_city}\n",
    "contact_instagram : {contact_instagram}\n",
    "image : {image}\n",
    "location : {location}\n",
    "outdoor_seating : {outdoor_seating}\n",
    "museum : {museum}\n",
    "takeaway : {takeaway}\n",
    "smoking : {smoking}\n",
    "name : {name}\n",
    "id : {id} \"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0d2c8a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(search_results):\n",
    "    \n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + entry_template.format(**doc) + \"\\n\\n\"\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c4ea2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(prompt_template, query, context):\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ae38a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_llm(prompt):\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0\n",
    "        # using temperature=0 ensures deterministic outputs.\n",
    "# This is important when comparing models/prompts,\n",
    "# because it removes randomness from the generation.\n",
    "# That way, differences come only from the model/prompt,\n",
    "# not from sampling noise.\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8e367fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_rrf_results(results):\n",
    "    context_selected_ids = []\n",
    "    for record in results:\n",
    "        context_selected_ids.append(record.id)\n",
    "    return [doc for doc in documents if doc[\"id\"] in context_selected_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2de5614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rag(llm, query, prompt_template):\n",
    "    search_results = rrf_search(query)\n",
    "    search_results =filter_rrf_results(search_results)\n",
    "    context = build_context(search_results)\n",
    "    prompt = build_prompt(prompt_template,query, context)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "413c3e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "\n",
    "def mistral_llm(prompt):\n",
    "    mistral_client = Mistral(api_key=MISTRAL_API_KEY)\n",
    "    response = \tmistral_client.chat.complete(\n",
    "        model=\"mistral-large-latest\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70dcf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {'openai_llm': openai_llm, 'mistral_llm': mistral_llm}\n",
    "\n",
    "PROMPTS = {\n",
    "    \"prompt_A\": \"\"\"### Act as a **Kraków travel expert** with access to a comprehensive POI (Points of Interest) database.\n",
    "Your role is to provide **precise, personalized, and actionable recommendations** for visitors by dynamically retrieving and synthesizing information from the POI database.\n",
    "\n",
    "---\n",
    "**Guidelines:**\n",
    "\n",
    "1. **Always query the POI database** for the most accurate and up-to-date information.\n",
    "\n",
    "2. **Filter POIs** based on:\n",
    "   - User preferences (e.g., historical sites, vegan food, family-friendly)\n",
    "   - Location (proximity to the user’s current area or planned route)\n",
    "   - Time of day/year (e.g., seasonal attractions, nightlife)\n",
    "   - Budget (free, mid-range, luxury)\n",
    "\n",
    "3. **Structure your response** as follows:\n",
    "   - **Direct Answer:** Start with a concise reply to the user’s question.\n",
    "   - **POI Details:** Include **name, address, opening hours, price range, and a brief description** for each relevant POI.\n",
    "   - **Contextual Tips:** Add practical advice (e.g., best time to visit, how to skip lines, nearby (geometry in context) POIs to combine).\n",
    "   - **Personalization:** Tailor suggestions to the user’s interests, group size, and mobility.\n",
    "\n",
    "4. **Proactive Suggestions:**\n",
    "   - If the user mentions a POI, suggest nearby attractions or activities (e.g., *\"After visiting Schindler’s Factory, you can walk to the MOCAK Museum or explore Kazimierz.\"*).\n",
    "   - For multi-day trips, offer itinerary templates based on POI clusters (e.g., *\"Day 1: Old Town + Wawel; Day 2: Kazimierz + Podgórze\"*).\n",
    "\n",
    "5. **Geospatial Queries:**\n",
    "   - Use POI coordinates to suggest walking routes or clusters.\n",
    "\n",
    "6. **Check**\n",
    "   - Use context strictly.\n",
    "   \n",
    "   QUESTION: {question}\n",
    "\n",
    "   CONTEXT: {context}\n",
    "   \n",
    "   Answer:\"\"\",\n",
    "\n",
    "\n",
    "    \"prompt_B\": \"\"\"### You are a Kraków Travel Assistant specialized in Points of Interest (POIs). Your goal is to provide accurate, actionable, \n",
    "    and personalized recommendations using both your knowledge and retrieved documents.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Evaluate retrieved POIs for relevance:\n",
    "\n",
    "Prioritize attractions that match the user’s interests, duration of stay, and location preferences.\n",
    "\n",
    "Rank POIs by importance, popularity, uniqueness, or cultural value.\n",
    "\n",
    "For each recommended POI, include:\n",
    "\n",
    "Name, location, and historical or cultural significance\n",
    "\n",
    "Visiting hours, entry fees, and booking tips\n",
    "\n",
    "Nearby attractions (geometry in context), restaurants, or cafes\n",
    "\n",
    "Transportation options and accessibility details\n",
    "\n",
    "Suggested activities or must-see highlights\n",
    "\n",
    "Presentation style:\n",
    "\n",
    "Friendly, engaging, and easy-to-follow\n",
    "\n",
    "Provide concise lists or itineraries when relevant\n",
    "\n",
    "Clearly indicate when information comes from retrieved sources\n",
    "\n",
    "Flag any uncertainty or missing information\n",
    "\n",
    "Personalization:\n",
    "\n",
    "Tailor recommendations to visitor preferences (e.g., history, food, art, walking tours)\n",
    "\n",
    "Suggest optimal itineraries based on number of days in Kraków\n",
    "\n",
    "Always aim to help the user explore Kraków efficiently and enjoyably, focusing on the most relevant and high-value POIs.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "Answer:\"\"\"\n",
    ",\n",
    "'prompt_C': \"\"\"### You are a Krakow travel assistant and expert guide. \n",
    "Your task is to answer the QUESTION based **strictly** on the information provided in the CONTEXT.\n",
    "Do not use any external knowledge or make assumptions — rely only on the facts from the CONTEXT.\n",
    "\n",
    "- Be clear and concise.\n",
    "- Make your answer complete but do not add information not in the CONTEXT.\n",
    "- If the CONTEXT does not contain the answer, say: \"I don't have enough information to answer that.\"\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "}\n",
    "TEMPERATURE = 0.0\n",
    "\n",
    "# --- helper: call LLM-judge to label one sample (use your judge prompt) ---\n",
    "JUDGE_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an evaluator. Your task is to classify the quality of the answer provided by a RAG system.\n",
    "Return ONLY JSON with labels (no extra text). Use one of the allowed labels for each criterion.\n",
    "\n",
    "Faithfulness: [\"NON_FAITHFUL\",\"PARTLY_FAITHFUL\",\"FAITHFUL\"]\n",
    "Groundedness: [\"NON_GROUNDED\",\"PARTLY_GROUNDED\",\"GROUNDED\"]\n",
    "Relevance: [\"NON_RELEVANT\",\"PARTLY_RELEVANT\",\"RELEVANT\"]\n",
    "Completeness: [\"NON_COMPLETE\",\"PARTLY_COMPLETE\",\"COMPLETE\"]\n",
    "Coherence: [\"NON_COHERENT\",\"PARTLY_COHERENT\",\"COHERENT\"]\n",
    "Conciseness: [\"NON_CONCISE\",\"PARTLY_CONCISE\",\"CONCISE\"]\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer: {answer}\n",
    "\n",
    "Return JSON exactly like:\n",
    "{{\"faithfulness\":\"...\", \"groundedness\":\"...\", \"relevance\":\"...\", \"completeness\":\"...\", \"coherence\":\"...\", \"conciseness\":\"...\"}}\n",
    "\"\"\"\n",
    "\n",
    "def judge_label(question, context, answer):\n",
    "    prompt = JUDGE_PROMPT_TEMPLATE.format(question=question, context=context, answer=answer)\n",
    "    resp = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # LLM-sędzia\n",
    "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    text = resp.choices[0].message.content.strip()\n",
    "    \n",
    "    # usuń ewentualne znaczniki markdown ```json ... ```\n",
    "    text = re.sub(r\"```json|```\", \"\", text).strip()\n",
    "    \n",
    "    try:\n",
    "        labels = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        # fallback: ewaluacja literalna (bez markdown)\n",
    "        try:\n",
    "            labels = eval(text)\n",
    "        except Exception as e:\n",
    "            print(\"Failed to parse JSON from model response:\", repr(text))\n",
    "            raise e\n",
    "    return labels\n",
    "\n",
    "# --- helper: convert labels to numeric quality_score (0-6) ---\n",
    "POSITIVE_MAPPING = {\n",
    "    \"faithfulness\": \"FAITHFUL\",\n",
    "    \"groundedness\": \"GROUNDED\",\n",
    "    \"relevance\": \"RELEVANT\",\n",
    "    \"completeness\": \"COMPLETE\",\n",
    "    \"coherence\": \"COHERENT\",\n",
    "    \"conciseness\": \"CONCISE\"\n",
    "}\n",
    "\n",
    "def quality_score_from_labels(labels):\n",
    "    score = 0\n",
    "    for crit, pos_label in POSITIVE_MAPPING.items():\n",
    "        if labels.get(crit) == pos_label:\n",
    "            score += 1\n",
    "    return score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "406e95c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3340\n"
     ]
    }
   ],
   "source": [
    "print(len(ground_truth))\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "ground_truth_sample = random.sample(ground_truth, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0dbc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: model=openai_llm prompt=prompt_A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [03:10<44:31, 28.73s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "records = []\n",
    "\n",
    "for model_name, model in MODELS.items():\n",
    "    for prompt_name, prompt_template in PROMPTS.items():\n",
    "        print(f\"Running: model={model_name} prompt={prompt_name}\")\n",
    "        for record in tqdm(ground_truth_sample):\n",
    "            question = record[\"question\"]\n",
    "            # 1) generate answer (cache to avoid powtarzania)\n",
    "            cache_key = f\"{model_name}__{prompt_name}__{record['id']}.json\"\n",
    "            cache_path = os.path.join(OUTPUT_DIR, cache_key)\n",
    "            if os.path.exists(cache_path):\n",
    "                with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    rec = json.load(f)\n",
    "            else:\n",
    "                search_results = rrf_search(question)\n",
    "                search_results =filter_rrf_results(search_results)\n",
    "                context = build_context(search_results)\n",
    "                prompt = build_prompt(prompt_template,question, context)\n",
    "                answer = model(prompt)\n",
    "                labels = judge_label(question, context, answer)\n",
    "                score = quality_score_from_labels(labels)\n",
    "                rec = {\n",
    "                    \"model\":    model_name,\n",
    "                    \"prompt\": prompt_name,\n",
    "                    \"id\": record[\"id\"],\n",
    "                    \"question\": question,\n",
    "                    \"context\": context,\n",
    "                    \"answer\": answer,\n",
    "                    \"labels\": labels,\n",
    "                    \"quality_score\": score\n",
    "                }\n",
    "                with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(rec, f, ensure_ascii=False, indent=2)\n",
    "            records.append(rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aa08e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.to_parquet(os.path.join(OUTPUT_DIR, \"all_runs.parquet\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc7873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- ANALIZA POROWNAWCZA ----------\n",
    "# 1) dystrybucje quality_score per (model,prompt)\n",
    "summary = df.groupby([\"model\",\"prompt\"])[\"quality_score\"].describe()\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910653cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) rozkład etykiet relevance per pair\n",
    "rel_dist = df.groupby([\"model\",\"prompt\"])[\"labels\"].apply(lambda s: pd.Series([x[\"relevance\"] for x in s]).value_counts(normalize=True))\n",
    "print(rel_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Entropia rozkładów etykiet (relevance)\n",
    "def entropy_for_group(g):\n",
    "    vals = [x[\"relevance\"] for x in g[\"labels\"]]\n",
    "    counts = pd.Series(vals).value_counts(normalize=True)\n",
    "    return entropy(counts, base=2)\n",
    "\n",
    "ent = df.groupby([\"model\",\"prompt\"]).apply(entropy_for_group)\n",
    "print(\"Entropy (relevance):\\n\", ent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a133f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Bootstrap: porównanie średniego quality_score między dwoma konfiguracjami\n",
    "def bootstrap_mean_diff(a, b, n_boot=5000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    diffs = []\n",
    "    n = len(a)\n",
    "    for _ in range(n_boot):\n",
    "        idx = rng.randint(0, n, n)  # bootstrap indices\n",
    "        diffs.append(np.mean(a[idx]) - np.mean(b[idx]))\n",
    "    diffs = np.array(diffs)\n",
    "    lo, hi = np.percentile(diffs, [2.5, 97.5])\n",
    "    return np.mean(a) - np.mean(b), lo, hi\n",
    "\n",
    "# example: compare first two combos\n",
    "combos = list(df.groupby([\"model\",\"prompt\"]))\n",
    "if len(combos) >= 2:\n",
    "    (m1,p1), g1 = combos[0]\n",
    "    (m2,p2), g2 = combos[1]\n",
    "    # ensure matched ids: join on sample id\n",
    "    merged = pd.merge(g1, g2, on=\"id\", suffixes=(\"_a\",\"_b\"))\n",
    "    a = merged[\"quality_score_a\"].values\n",
    "    b = merged[\"quality_score_b\"].values\n",
    "    mean_diff, lo, hi = bootstrap_mean_diff(a, b)\n",
    "    print(f\"Mean quality_score diff ({m1},{p1}) - ({m2},{p2}) = {mean_diff:.3f}, 95% CI [{lo:.3f}, {hi:.3f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c4e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save final aggregated results\n",
    "agg = df.groupby([\"model\",\"prompt\"])[\"quality_score\"].agg([\"mean\",\"std\",\"count\"])\n",
    "agg.to_csv(os.path.join(OUTPUT_DIR,\"agg_results.csv\"))\n",
    "\n",
    "print(agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29451954",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Załóżmy, że masz df jak wcześniej:\n",
    "# kolumny: model, prompt, labels (dict z 'faithfulness','groundedness','relevance',...)\n",
    "\n",
    "# --- przygotowanie danych do wykresu ---\n",
    "# Rozbijamy kolumnę 'labels' na osobne wiersze dla każdego kryterium\n",
    "records_plots = []\n",
    "for _, row in df.iterrows():\n",
    "    for crit in ['faithfulness','groundedness','relevance']:\n",
    "        records_plots .append({\n",
    "            'model': row['model'],\n",
    "            'prompt': row['prompt'],\n",
    "            'criterion': crit,\n",
    "            'label': row['labels'][crit]\n",
    "        })\n",
    "\n",
    "plot_df = pd.DataFrame(records_plots)\n",
    "\n",
    "# --- wykresy słupkowe dla każdej konfiguracji ---\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "for crit in ['faithfulness','groundedness','relevance']:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    subset = plot_df[plot_df['criterion'] == crit]\n",
    "    \n",
    "    # liczność etykiet\n",
    "    sns.countplot(data=subset, x='label', hue='model', palette='Set2')\n",
    "    \n",
    "    plt.title(f'Distribution of labels for {crit}')\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='Model')\n",
    "    plt.show()\n",
    "\n",
    "# --- wykres procentowy ---\n",
    "for crit in ['faithfulness','groundedness','relevance']:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    subset = plot_df[plot_df['criterion'] == crit]\n",
    "    \n",
    "    # obliczamy procenty\n",
    "    pct_df = subset.groupby(['model','prompt','label']).size().groupby(level=[0,1]).apply(lambda x: 100*x/x.sum()).reset_index(name='percent')\n",
    "    \n",
    "    sns.barplot(data=pct_df, x='label', y='percent', hue='model', palette='Set2')\n",
    "    plt.title(f'Percentage distribution of labels for {crit}')\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Percent (%)')\n",
    "    plt.legend(title='Model')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel_assitant-sAPdwvgP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
